{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rintsQul0c1l"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5100ed4249441e48a5a00d9ab624d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ec7ed74de28f48428d748f7a37ae3e73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_83cfd63c631a41909302388b35e47ccc",
              "IPY_MODEL_9de1fdee538c4d2fb20993f7807c8a59"
            ]
          }
        },
        "ec7ed74de28f48428d748f7a37ae3e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83cfd63c631a41909302388b35e47ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f67c130aa94e42c4b89ad41923ef63d8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_35ffc5d3fd0e4846891d5f2cdb16b703"
          }
        },
        "9de1fdee538c4d2fb20993f7807c8a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_113ec2249b1644e6a3c19c02171299bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 9.84kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d1f0f0b9549478489f3d49134ac62ce"
          }
        },
        "f67c130aa94e42c4b89ad41923ef63d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "35ffc5d3fd0e4846891d5f2cdb16b703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "113ec2249b1644e6a3c19c02171299bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d1f0f0b9549478489f3d49134ac62ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2b17cdb2f1846fe8d6b3bdb99263893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_285b2d7bd6d84c42842fbddf0e167c79",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_83f79c9c290c443b9339a288edd54726",
              "IPY_MODEL_cb67a7edf29b4392b09d97dfd9b64546"
            ]
          }
        },
        "285b2d7bd6d84c42842fbddf0e167c79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83f79c9c290c443b9339a288edd54726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d978ccb35524773900bb84d947ccab3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_32459abe664243f1ba0ec1ebf60f320c"
          }
        },
        "cb67a7edf29b4392b09d97dfd9b64546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eadafb820656437c95d7c163d32363ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 687kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3de94497ae76424499370fd4c94e6c58"
          }
        },
        "4d978ccb35524773900bb84d947ccab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "32459abe664243f1ba0ec1ebf60f320c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eadafb820656437c95d7c163d32363ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3de94497ae76424499370fd4c94e6c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9a1d7a224a6a4b62a6c24deafa17035a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3e95f2da97e34487948471f330215e19",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf031305c9af4fe598082bb794991f5f",
              "IPY_MODEL_3c18e8ea8d7444e3a8c1037e307e8fa0"
            ]
          }
        },
        "3e95f2da97e34487948471f330215e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf031305c9af4fe598082bb794991f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3cc3396fdd1241b0978872d13123226c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a8e5e1ac2b34a3d92b4a3678a6d18e5"
          }
        },
        "3c18e8ea8d7444e3a8c1037e307e8fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c00a8a1ae0343c183bff8c8811031ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:06&lt;00:00, 69.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4103223b7d63416b8be0182fd4a63cdc"
          }
        },
        "3cc3396fdd1241b0978872d13123226c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a8e5e1ac2b34a3d92b4a3678a6d18e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c00a8a1ae0343c183bff8c8811031ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4103223b7d63416b8be0182fd4a63cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jordy-VL/document-classification-exps/blob/master/BERT_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWwzkgM5oNMH",
        "colab_type": "text"
      },
      "source": [
        "#BERT for document classification\n",
        "*Welcome to the BERT tutorial*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHgyWugytlu",
        "colab_type": "text"
      },
      "source": [
        "## Installation\n",
        "*And of course some necessary packages to install in our Colab environment*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_svKGYyztx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "cb7143bb-f2f8-48e4-9729-6cbb45810a5e"
      },
      "source": [
        "!pip3 install tqdm matplotlib scikit-learn keras\n",
        "!pip3 install torch -i https://download.pytorch.org/whl/cu90/torch_stable.html/torch-1.0.0-cp36-cp36m-linux_x86_64.whl \n",
        "!pip3 install transformers #pytorch_transformers\n",
        "# https://colab.research.google.com/drive/1uvHuizCBqFgvbCwEhK7FvU8JW0AfxgJw install custom module"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.15.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu90/torch_stable.html/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.7MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=cde313c483269a3fecd559686064c1ee51c765fdb22b7faa97b3bb8fa57bd4cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ae6FHE2jozu",
        "colab_type": "text"
      },
      "source": [
        "### Enter free GPU\n",
        "Let's fire up a free GPU by going to \"edit - notebook settings\" and choosing \"GPU\" as hardware accelerator. The below script will give you some statistics on the device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hlWoGRYjzMd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9fe3d278-9c40-4873-faed-30332faeaba3"
      },
      "source": [
        "!nvidia-smi\n",
        "#for TPUs: https://colab.research.google.com/drive/1M8uYeHHQjmomsSEZJ6NNtfpEL_hPzcpq#scrollTo=AoJ4XQWoHbIB "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul  6 10:52:38 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.36.06    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPUrj8bzttPx",
        "colab_type": "text"
      },
      "source": [
        "### Mount google drive \n",
        "(required for saving model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BT73ml-tkLj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "1c1089b9-6b1e-4ec6-850d-3cdac82cf1d0"
      },
      "source": [
        "#https://medium.com/@ml_kid/how-to-save-our-model-to-google-drive-and-reuse-it-2c1028058cb2 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path='/content/gdrive/My Drive'\n",
        "!path=\"/content/gdrive/My Drive\"\n",
        "!ls \"$path\"\n",
        "!mkdir -p \"$path\"\n",
        "\n",
        "#bonus: import module from Google Drive\n",
        "\"\"\"\n",
        "import sys\n",
        "sys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/my_modules')\n",
        "from woef import main as waf\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "'BenchmarkingZeroShotData.tar.gz (Unzipped Files)'\n",
            "'Colab Notebooks'\n",
            " collect\n",
            "'Getting started.pdf'\n",
            "'Google Photos'\n",
            " JordyVanLandeghem_Resume-July2019.pdf\n",
            " models\n",
            " models.zip\n",
            "'Reports ANN'\n",
            " Sicilia\n",
            "'ubuntu.zip (Unzipped Files)'\n",
            "'udacity.zip (Unzipped Files)'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled presentation.gslides'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nimport sys\\nsys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/my_modules')\\nfrom woef import main as waf\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVGZL8PZTcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper imports\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "#from evaluate import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuxgxPfGjnXj",
        "colab_type": "text"
      },
      "source": [
        "let's start with some hardcoded values to ensure we run the same strategy achieving reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL7lEsz2dSms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23e140fc-4b14-448e-9772-6fb9af8cc7ae"
      },
      "source": [
        "SEED = 100\n",
        "np.random.seed(SEED)\n",
        "sample = 0 # set to XXX in order to perform input sampling on train/val/test (enabling dryrun mode)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPJmyjf8oktV",
        "colab_type": "text"
      },
      "source": [
        "## Loading train/test documents\n",
        "### 20-News Corpus \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ6O1mjkpIOj",
        "colab_type": "text"
      },
      "source": [
        "Below we fetch the 20News corpus as a databunch from sklearn and setup X (features) and y (labels) for train and test. To ensure we run all the same preprocessing steps for the different sets, we put them per identifier in a dictionary. From experience, this helps to reduce code duplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7y-5L5IpfDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identifiers = [\"train\", \"val\", \"test\"]\n",
        "data = {identifier: {} for identifier in identifiers}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdSX189Aoi0E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "423e7ba2-fd63-4af0-f863-22ed058422d5"
      },
      "source": [
        "##sample dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train', categories=None,\n",
        "                                shuffle=True, random_state=42)\n",
        "data_test = fetch_20newsgroups(subset='test', categories=None,\n",
        "                               shuffle=True, random_state=42)\n",
        "\n",
        "# order of labels in `target_names` can be different from `categories`\n",
        "labels = data_train.target_names\n",
        "num_labels = len(labels)\n",
        "\n",
        "data[\"train\"][\"X\"], data[\"test\"][\"X\"] = data_train.data, data_test.data\n",
        "data[\"train\"][\"y\"], data[\"test\"][\"y\"] = data_train.target, data_test.target"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U50NOsZf-GU9",
        "colab_type": "text"
      },
      "source": [
        "We can also download one of our proprietary datasets and input them in the same fashion for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymm5_F3VpXoj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "bc8ae525-3728-4347-9b8e-d303b16ff703"
      },
      "source": [
        " ##real dataset\n",
        "  #https://github.com/ctberthiaume/gdcp\n",
        "from google.colab import files\n",
        "#files.upload()\n",
        "\n",
        "#optionally move it to gdrive\n",
        "datapath = \"XXX.csv\"\n",
        "import pandas as pd\n",
        "df = pd.read_csv(datapath)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-13cf11b55561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdatapath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"191004_AMMA_prep.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File 191004_AMMA_prep.csv does not exist: '191004_AMMA_prep.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWhLl2WIGWY5",
        "colab_type": "text"
      },
      "source": [
        "Only run this cell if you use a custom dataset; adjust like required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF1nHiIaxDyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "087b39be-9903-4e83-96d4-2bfde2b97653"
      },
      "source": [
        "#print(df.head())\n",
        "#print(df.columns.tolist())\n",
        "tag = \"Doctype\"\n",
        "num_labels = len(df[tag].unique())\n",
        "labels = sorted(df[tag].unique().tolist())\n",
        "mapping = {k:i for i,k in enumerate(sorted(df[tag].unique().tolist()))}\n",
        "df[tag] = df[tag].apply(lambda x: mapping[x])\n",
        "data[\"train\"][\"X\"], data[\"test\"][\"X\"], data[\"train\"][\"y\"], data[\"test\"][\"y\"] = train_test_split(df[\"unprep\"].values, df[tag].values,\n",
        "                                                                                                        random_state=SEED, test_size=0.3)\n",
        "# data[\"train\"][\"X\"], data[\"val\"][\"X\"], data[\"train\"][\"y\"], data[\"val\"][\"y\"] = train_test_split(data[\"train\"][\"X\"], data[\"train\"][\"y\"],\n",
        "#                                                                                                         random_state=SEED, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5380029249d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(df.columns.tolist())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Doctype\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478aqnniojd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if sample:\n",
        "    for l in [\"train\", \"test\"]:\n",
        "        for k in [\"X\", \"y\"]:\n",
        "            data[l][k] = data[l][k][:sample+1]\n",
        "\n",
        "# Set the maximum sequence length. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 256\n",
        "# Optional function to find MAX_LEN: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqH_6nJ97St",
        "colab_type": "text"
      },
      "source": [
        "## Loading pre-trained BERT weights and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdeZVUWo7gc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "b5100ed4249441e48a5a00d9ab624d7d",
            "ec7ed74de28f48428d748f7a37ae3e73",
            "83cfd63c631a41909302388b35e47ccc",
            "9de1fdee538c4d2fb20993f7807c8a59",
            "f67c130aa94e42c4b89ad41923ef63d8",
            "35ffc5d3fd0e4846891d5f2cdb16b703",
            "113ec2249b1644e6a3c19c02171299bf",
            "5d1f0f0b9549478489f3d49134ac62ce",
            "f2b17cdb2f1846fe8d6b3bdb99263893",
            "285b2d7bd6d84c42842fbddf0e167c79",
            "83f79c9c290c443b9339a288edd54726",
            "cb67a7edf29b4392b09d97dfd9b64546",
            "4d978ccb35524773900bb84d947ccab3",
            "32459abe664243f1ba0ec1ebf60f320c",
            "eadafb820656437c95d7c163d32363ef",
            "3de94497ae76424499370fd4c94e6c58"
          ]
        },
        "outputId": "ab78d1f8-7e07-4b58-dc9e-608437feb3e6"
      },
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer,AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
        "model_class, tokenizer_class, pretrained_weights = BertForSequenceClassification, BertTokenizer, \"bert-base-uncased\" #'bert-base-multilingual-cased' #allenai/longformer-base-4096\n",
        "#tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model_class = \"bert-base-uncased\"#'allenai/longformer-base-4096'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5100ed4249441e48a5a00d9ab624d7d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2b17cdb2f1846fe8d6b3bdb99263893",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUArAej2TpVP",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization - Max Length Trim/Pad + Encode - BatchLoader -> train/val/test  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZLu03z0fQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a652f9ab-eb14-4ac4-8019-5b81795b1d2b"
      },
      "source": [
        "for l in [\"train\", \"test\"]:\n",
        "    print(\"l: \", l)\n",
        "    data[l][\"tokenized\"] = tqdm([tokenizer.tokenize(text)[:MAX_LEN]\n",
        "                                 for text in data[l][\"X\"]])  # [:MAX_LEN]\n",
        "    data[l][\"input_ids\"] = pad_sequences(tqdm([tokenizer.convert_tokens_to_ids(tokenized) for tokenized in data[l][\"tokenized\"]]),\n",
        "                                         maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    # Create attention masks\n",
        "    data[l][\"masks\"] = []\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in data[l][\"input_ids\"]:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        data[l][\"masks\"].append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l:  train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11314/11314 [00:03<00:00, 3372.92it/s]\n",
            "100%|██████████| 11314/11314 [00:00<00:00, 1367403.05it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "l:  test\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7532/7532 [00:02<00:00, 3242.16it/s]\n",
            "100%|██████████| 7532/7532 [00:00<00:00, 1874866.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K72fG5MeUhek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b22dd6c4-aa1f-41b4-ac17-b9c0c560474b"
      },
      "source": [
        "data[\"train\"][\"inputs\"], data[\"val\"][\"inputs\"], data[\"train\"][\"y\"], data[\"val\"][\"y\"] = train_test_split(data[\"train\"][\"input_ids\"], data[\"train\"][\"y\"],\n",
        "                                                                                                        random_state=SEED, test_size=0.1)\n",
        "data[\"train\"][\"masks\"], data[\"val\"][\"masks\"], _, _ = train_test_split(data[\"train\"][\"masks\"], data[\"train\"][\"input_ids\"],\n",
        "                                                                      random_state=SEED, test_size=0.1)\n",
        "data[\"test\"][\"inputs\"] = data[\"test\"][\"input_ids\"]\n",
        "\n",
        "# TORCHify arrays and matrices\n",
        "for l in [\"train\", \"val\", \"test\"]:\n",
        "    print(l)\n",
        "    for k in [\"inputs\", \"y\", \"masks\"]:\n",
        "        print(k)\n",
        "        data[l][k] = torch.tensor(data[l][k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "inputs\n",
            "y\n",
            "masks\n",
            "val\n",
            "inputs\n",
            "y\n",
            "masks\n",
            "test\n",
            "inputs\n",
            "y\n",
            "masks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUw-s5BnUP--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOADERs\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(data[\"train\"][\"inputs\"], data[\"train\"][\"masks\"], data[\"train\"][\"y\"])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(data[\"val\"][\"inputs\"], data[\"val\"][\"masks\"], data[\"val\"][\"y\"])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(\n",
        "    val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(data[\"test\"][\"inputs\"], data[\"test\"][\"masks\"], data[\"test\"][\"y\"])\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKDwQvfYUXxL",
        "colab_type": "text"
      },
      "source": [
        "## Training & Optimization params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NP1lz_I0ffU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "EPOCHS = 2\n",
        "if sample: EPOCHS = 1\n",
        "lr = 3e-5 #1e-6\n",
        "max_grad_norm = 1.0\n",
        "warmup_proportion = 0.1\n",
        "num_total_steps = len(train_dataloader) #*EPOCHS\n",
        "num_warmup_steps = float(num_total_steps)*0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsix0VY_VPiq",
        "colab_type": "text"
      },
      "source": [
        "## Fire up the model on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYA0OGGHU9pH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "9a1d7a224a6a4b62a6c24deafa17035a",
            "3e95f2da97e34487948471f330215e19",
            "bf031305c9af4fe598082bb794991f5f",
            "3c18e8ea8d7444e3a8c1037e307e8fa0",
            "3cc3396fdd1241b0978872d13123226c",
            "4a8e5e1ac2b34a3d92b4a3678a6d18e5",
            "1c00a8a1ae0343c183bff8c8811031ef",
            "4103223b7d63416b8be0182fd4a63cdc"
          ]
        },
        "outputId": "513d534c-0d5f-4462-ac13-6d1cafe6256b"
      },
      "source": [
        "#model = model_class.from_pretrained(pretrained_weights, num_labels=num_labels)\n",
        "config = AutoConfig.from_pretrained(model_class,num_labels=num_labels)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_class,config=config)\n",
        "model.cuda()\n",
        "# To reproduce BertAdam specific behavior set correct_bias=False\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps = num_total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a1d7a224a6a4b62a6c24deafa17035a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDFP1ajfsa3L",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZqBhhQgsfb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, brier_score_loss, precision_recall_curve,roc_curve,auc,roc_auc_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "def evaluation_measures(gold, predicted, target_names=[]):\n",
        "    acc = accuracy_score(gold,predicted)\n",
        "    print('Accuracy [:( metric ]:', acc)\n",
        "    if target_names:\n",
        "        report = classification_report(gold, predicted, target_names=target_names)\n",
        "        repdict = classification_report(gold, predicted, target_names=target_names, output_dict=True)\n",
        "    else:\n",
        "        report = classification_report(gold, predicted)\n",
        "        repdict = classification_report(gold, predicted, output_dict=True)\n",
        "    print('Classification report:')\n",
        "    print(report)\n",
        "    matrix = confusion_matrix(gold, predicted)\n",
        "    print('Confusion matrix:')\n",
        "    print(matrix)\n",
        "    return repdict, matrix\n",
        "\n",
        "def calc_uof_fp(points, thresh):\n",
        "    stats = {}\n",
        "    stats[\"pos_over\"], stats[\"pos_under\"], stats[\"neg_over\"], stats[\"neg_under\"] = 0, 0, 0, 0\n",
        "\n",
        "    multiply = True\n",
        "    for i, p, value, status, group in points:\n",
        "        if value > 1:\n",
        "            multiply = False\n",
        "            break\n",
        "    if multiply:\n",
        "        points = [(i, p, 100*value, status, group) for i, p, value, status, group in points]\n",
        "\n",
        "    for i, p, value, status, group in points:\n",
        "        if status == True:\n",
        "            if value >= thresh:\n",
        "                stats[\"pos_over\"] += 1\n",
        "            else:\n",
        "                stats[\"pos_under\"] += 1\n",
        "        else:\n",
        "            if value >= thresh:\n",
        "                stats[\"neg_over\"] += 1\n",
        "            else:\n",
        "                stats[\"neg_under\"] += 1\n",
        "    return round((stats[\"pos_over\"] + stats[\"neg_over\"])/len(points), 4), round(stats[\"neg_over\"]/max(1, ((stats[\"pos_over\"] + stats[\"neg_over\"]))), 2), round((stats[\"pos_over\"])/len(points), 4)\n",
        "\n",
        "def easy_calc_uof_fp(predict, probs, gold, thresh):\n",
        "    \"\"\"\n",
        "    LIST all\n",
        "    \"\"\"\n",
        "    total = len(gold)\n",
        "    boolean = np.array([True if predict[i] == gold[i] else False for i in range(0, len(gold))])\n",
        "    unique = sorted(list(set(sorted([int(x) for x in probs]))))\n",
        "    points = list(zip(list(range(0, total)), predict, probs, boolean.tolist(),\n",
        "                      [unique.index(int(prob)) for prob in probs]))\n",
        "    return calc_uof_fp(points, thresh)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX1n0Jc_rlVA",
        "colab_type": "text"
      },
      "source": [
        "## Train - Validate Model & Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMjB2YNrkyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "94aa6842-f312-4a1b-e1b3-6341a2d5a65a"
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "\n",
        "    # Training\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    #from pdb import set_trace; set_trace()\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        # print(loss)\n",
        "\n",
        "        #from pdb import set_trace; set_trace()\n",
        "        train_loss_set.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Update learning rate for next steps\n",
        "        scheduler.step()\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the val set\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up val\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits = outputs[0] # PER BATCH!\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"val Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "\n",
        "# Now let's save our model and tokenizer to a directory\n",
        "model.save_pretrained(path+'/models/')\n",
        "tokenizer.save_pretrained(path+'/models/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.9627256820070538\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rEpoch:  50%|█████     | 1/2 [35:56<35:56, 2156.58s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Accuracy: 0.840669014084507\n",
            "Train loss: 0.4234565642412872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 2/2 [1:11:54<00:00, 2157.17s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "val Accuracy: 0.840669014084507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/gdrive/My Drive/models/vocab.json',\n",
              " '/content/gdrive/My Drive/models/merges.txt',\n",
              " '/content/gdrive/My Drive/models/special_tokens_map.json',\n",
              " '/content/gdrive/My Drive/models/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bP9PPuvVeR",
        "colab_type": "text"
      },
      "source": [
        "## Test - evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHa16XYEbfm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions, probs, true_labels = [], [], []\n",
        "\n",
        "# Predict\n",
        "for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "        prob = F.softmax(logits, dim=1)\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    prob = prob.detach().cpu().numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    probs.append(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHbx_fst0fos",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation & Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kWOep3kvZgg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9e45897-d525-4669-c5de-4a9d48ec4bd2"
      },
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "matthews_set = []\n",
        "for i in range(len(true_labels)):\n",
        "    matthews = matthews_corrcoef(true_labels[i],\n",
        "                                np.argmax(predictions[i], axis=1).flatten())\n",
        "    matthews_set.append(matthews)\n",
        "#print(matthews_set)\n",
        "\n",
        "#from pdb import set_trace; set_trace()\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_true_labels = [label for batch in true_labels for label in batch]\n",
        "flat_logits = [logits for batch in predictions for logits in batch]\n",
        "flat_probs = [prob for batch in probs for prob in batch]\n",
        "flat_predictions = np.argmax(flat_logits, axis=1).flatten()\n",
        "flat_argmax_probs = [100*flat_probs[i][flat_predictions[i]] for i in range(len(flat_predictions))]\n",
        "\n",
        "print(\"Matthews Coëfficient: \",matthews_corrcoef(flat_true_labels, flat_predictions))\n",
        "\n",
        "evaluation_measures(flat_true_labels, flat_predictions)#,target_names=labels)\n",
        "\n",
        "for thresh in np.arange(10,100,10):\n",
        "    stats = easy_calc_uof_fp(flat_predictions, flat_argmax_probs, flat_true_labels, thresh)\n",
        "    print(thresh, \":\", stats)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Matthews Coëfficient:  0.7677883814439873\n",
            "Accuracy [:( metric ]: 0.7792087095061073\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.55      0.53      0.54       319\n",
            "           1       0.81      0.71      0.76       389\n",
            "           2       0.76      0.79      0.77       394\n",
            "           3       0.60      0.73      0.66       392\n",
            "           4       0.83      0.73      0.77       385\n",
            "           5       0.83      0.86      0.84       395\n",
            "           6       0.88      0.87      0.88       390\n",
            "           7       0.86      0.83      0.84       396\n",
            "           8       0.82      0.78      0.80       398\n",
            "           9       0.91      0.93      0.92       397\n",
            "          10       0.97      0.93      0.95       399\n",
            "          11       0.88      0.87      0.87       396\n",
            "          12       0.72      0.72      0.72       393\n",
            "          13       0.89      0.95      0.92       396\n",
            "          14       0.88      0.93      0.90       394\n",
            "          15       0.64      0.87      0.73       398\n",
            "          16       0.64      0.76      0.69       364\n",
            "          17       0.94      0.76      0.84       376\n",
            "          18       0.62      0.53      0.57       310\n",
            "          19       0.34      0.20      0.26       251\n",
            "\n",
            "    accuracy                           0.78      7532\n",
            "   macro avg       0.77      0.76      0.76      7532\n",
            "weighted avg       0.78      0.78      0.78      7532\n",
            "\n",
            "Confusion matrix:\n",
            "[[169   0   0   0   0   0   1   0   2   0   0   0   1   5   6  70  18   2\n",
            "    2  43]\n",
            " [  0 277  13  18   7  35   2   0   0   1   1  15  10   4   3   2   0   1\n",
            "    0   0]\n",
            " [  0  17 311  25  14  17   2   0   1   0   0   2   0   2   1   1   0   0\n",
            "    1   0]\n",
            " [  0   7  39 286  14   4   6   0   6   0   0   3  26   0   0   0   0   1\n",
            "    0   0]\n",
            " [  0   3   8  72 280   2   7   0   1   0   0   0   9   0   3   0   0   0\n",
            "    0   0]\n",
            " [  0  17  25   1   1 338   2   0   5   0   0   1   1   1   2   0   0   0\n",
            "    0   1]\n",
            " [  0   2   2  19   5   0 341   1   2   1   0   0  14   1   2   0   0   0\n",
            "    0   0]\n",
            " [  0   0   2   2   0   5   9 328  29   0   0   1  14   0   1   0   3   0\n",
            "    2   0]\n",
            " [  1   0   0   0   0   1   2  41 311   2   0   3  14   0   4   1   5   0\n",
            "   10   3]\n",
            " [  0   1   4   0   0   0   3   0   3 368  10   1   0   1   1   1   1   0\n",
            "    3   0]\n",
            " [  0   0   0   0   0   0   0   0   1  24 372   1   0   0   0   0   0   0\n",
            "    1   0]\n",
            " [  0   2   0   5   1   2   1   0   0   0   0 344   7   2   6   0  10   0\n",
            "   16   0]\n",
            " [  0   8   4  47  13   2   5   5  11   0   0   8 281   7   1   0   1   0\n",
            "    0   0]\n",
            " [  3   4   1   0   0   1   2   0   1   0   0   0   3 375   1   1   1   0\n",
            "    3   0]\n",
            " [  1   6   0   1   2   1   0   0   1   1   0   0   7   3 365   0   1   0\n",
            "    5   0]\n",
            " [ 28   0   0   0   0   0   0   0   0   0   0   0   1   1   0 345   1   0\n",
            "    4  18]\n",
            " [  7   0   1   0   0   0   3   1   1   0   2   5   0   8   3   1 275   9\n",
            "   28  20]\n",
            " [ 50   0   1   0   0   0   1   1   2   3   0   2   0   0   1   2   3 287\n",
            "   20   3]\n",
            " [  3   0   0   1   1   0   1   1   0   2   0   4   2   9  11   0  99   2\n",
            "  165   9]\n",
            " [ 44   0   0   0   0   0   0   3   0   1   0   1   2   3   5 119  12   2\n",
            "    8  51]]\n",
            "10 : (1.0, 0.22, 0.7792)\n",
            "20 : (0.9993, 0.22, 0.7792)\n",
            "30 : (0.9952, 0.22, 0.7781)\n",
            "40 : (0.9745, 0.21, 0.7723)\n",
            "50 : (0.9287, 0.19, 0.7554)\n",
            "60 : (0.8821, 0.17, 0.7357)\n",
            "70 : (0.8348, 0.15, 0.7122)\n",
            "80 : (0.7645, 0.12, 0.6742)\n",
            "90 : (0.6362, 0.09, 0.5817)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-Bh2oy0cEM",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations, you have now been converted to BERTology! ![alt text](https://i.ytimg.com/vi/odVtLluew-8/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uzr4v5KZnU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bonus: how to get your model from google drive to local disk\n",
        "# zip folder with model -> download zip (works BUT slow for large models)\n",
        "# https://stackoverflow.com/questions/53581023/google-colab-file-download-failed-to-fetch-error => make sure to enable third-party cookies + possible refresh\n",
        "\n",
        "from google.colab import files\n",
        "!ls \"$path\"\n",
        "#!zip -r \"$path\"/models.zip \"$path\"'/models/'\n",
        "files.download(path+'/models.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rintsQul0c1l",
        "colab_type": "text"
      },
      "source": [
        "This code and tutorial all borrows insights from the below sources: \n",
        "\n",
        "# Tutorials #\n",
        "\n",
        "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "https://medium.com/dsnet/running-pytorch-transformers-on-custom-datasets-717fd9e10fe2\n",
        "https://engineering.wootric.com/when-bert-meets-pytorch\n",
        "https://towardsdatascience.com/distilling-bert-models-with-spacy-277c7edc426c\n",
        "https://github.com/huggingface/pytorch-transformers#quick-tour-of-the-fine-tuningusage-scripts\n",
        "https://github.com/explosion/spacy-pytorch-transformers\n",
        "https://github.com/huggingface/pytorch-transformers\n",
        "https://github.com/fredriko/bert-tensorflow-pytorch-spacy-conversion\n",
        "https://arxiv.org/pdf/1905.05583.pdf #how to finetune\n",
        "https://www.kaggle.com/sharmilaupadhyaya/20newsgroup-classification-using-keras-bert-in-gpu \n",
        "https://colab.research.google.com/drive/1YSfscbb-g92m1vkYxY4IOVMWMfgfLLJD #tensorflow version on TPU\n",
        "https://colab.research.google.com/drive/1pS-eegmUz9EqXJw22VbVIHlHoXjNaYuc#scrollTo=JggjeDC9m2MH #BertViz repo\n",
        "https://www.kaggle.com/criscastromaya/cnn-for-nlp-in-keras [compare with CNN]\n",
        "\n",
        "https://github.com/sugi-chan/custom_bert_pipeline/blob/master/bert_pipeline.ipynb #phased method\n",
        "\n",
        "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/100661 #faster batch training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9A-pVZW0iiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}