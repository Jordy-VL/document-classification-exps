{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rintsQul0c1l"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jordy-VL/document-classification-exps/blob/master/BERT_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWwzkgM5oNMH",
        "colab_type": "text"
      },
      "source": [
        "#BERT for document classification\n",
        "*Welcome to the BERT tutorial*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSHgyWugytlu",
        "colab_type": "text"
      },
      "source": [
        "## Installation\n",
        "*And of course some necessary packages to install in our Colab environment*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS_svKGYyztx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tqdm matplotlib scikit-learn keras\n",
        "!pip3 install torch -i https://download.pytorch.org/whl/cu90/torch_stable.html/torch-1.0.0-cp36-cp36m-linux_x86_64.whl \n",
        "!pip3 install transformers #pytorch_transformers\n",
        "# https://colab.research.google.com/drive/1uvHuizCBqFgvbCwEhK7FvU8JW0AfxgJw install custom module"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ae6FHE2jozu",
        "colab_type": "text"
      },
      "source": [
        "### Enter free GPU\n",
        "Let's fire up a free GPU by going to \"edit - notebook settings\" and choosing \"GPU\" as hardware accelerator. The below script will give you some statistics on the device. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hlWoGRYjzMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi\n",
        "#for TPUs: https://colab.research.google.com/drive/1M8uYeHHQjmomsSEZJ6NNtfpEL_hPzcpq#scrollTo=AoJ4XQWoHbIB "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPUrj8bzttPx",
        "colab_type": "text"
      },
      "source": [
        "### Mount google drive \n",
        "(required for saving model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BT73ml-tkLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://medium.com/@ml_kid/how-to-save-our-model-to-google-drive-and-reuse-it-2c1028058cb2 \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path='/content/gdrive/My Drive'\n",
        "!path=\"/content/gdrive/My Drive\"\n",
        "!ls \"$path\"\n",
        "!mkdir -p \"$path\"\n",
        "\n",
        "#bonus: import module from Google Drive\n",
        "\"\"\"\n",
        "import sys\n",
        "sys.path.insert(0, '/content/gdrive/My Drive/Colab Notebooks/my_modules')\n",
        "from woef import main as waf\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVGZL8PZTcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper imports\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "#from evaluate import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuxgxPfGjnXj",
        "colab_type": "text"
      },
      "source": [
        "let's start with some hardcoded values to ensure we run the same strategy achieving reproducible results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL7lEsz2dSms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 100\n",
        "np.random.seed(SEED)\n",
        "sample = 0 # set to XXX in order to perform input sampling on train/val/test (enabling dryrun mode)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPJmyjf8oktV",
        "colab_type": "text"
      },
      "source": [
        "## Loading train/test documents\n",
        "### 20-News Corpus \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ6O1mjkpIOj",
        "colab_type": "text"
      },
      "source": [
        "Below we fetch the 20News corpus as a databunch from sklearn and setup X (features) and y (labels) for train and test. To ensure we run all the same preprocessing steps for the different sets, we put them per identifier in a dictionary. From experience, this helps to reduce code duplication."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7y-5L5IpfDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "identifiers = [\"train\", \"val\", \"test\"]\n",
        "data = {identifier: {} for identifier in identifiers}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdSX189Aoi0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##sample dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "data_train = fetch_20newsgroups(subset='train', categories=None,\n",
        "                                shuffle=True, random_state=42)\n",
        "data_test = fetch_20newsgroups(subset='test', categories=None,\n",
        "                               shuffle=True, random_state=42)\n",
        "\n",
        "# order of labels in `target_names` can be different from `categories`\n",
        "labels = data_train.target_names\n",
        "num_labels = len(labels)\n",
        "\n",
        "data[\"train\"][\"X\"], data[\"test\"][\"X\"] = data_train.data, data_test.data\n",
        "data[\"train\"][\"y\"], data[\"test\"][\"y\"] = data_train.target, data_test.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U50NOsZf-GU9",
        "colab_type": "text"
      },
      "source": [
        "We can also download one of our proprietary datasets and input them in the same fashion for preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymm5_F3VpXoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " ##real dataset\n",
        "  #https://github.com/ctberthiaume/gdcp\n",
        "from google.colab import files\n",
        "#files.upload()\n",
        "\n",
        "#optionally move it to gdrive\n",
        "datapath = \"XXX.csv\"\n",
        "import pandas as pd\n",
        "df = pd.read_csv(datapath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWhLl2WIGWY5",
        "colab_type": "text"
      },
      "source": [
        "Only run this cell if you use a custom dataset; adjust like required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF1nHiIaxDyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(df.head())\n",
        "#print(df.columns.tolist())\n",
        "tag = \"Doctype\"\n",
        "num_labels = len(df[tag].unique())\n",
        "labels = sorted(df[tag].unique().tolist())\n",
        "mapping = {k:i for i,k in enumerate(sorted(df[tag].unique().tolist()))}\n",
        "df[tag] = df[tag].apply(lambda x: mapping[x])\n",
        "data[\"train\"][\"X\"], data[\"test\"][\"X\"], data[\"train\"][\"y\"], data[\"test\"][\"y\"] = train_test_split(df[\"unprep\"].values, df[tag].values,\n",
        "                                                                                                        random_state=SEED, test_size=0.3)\n",
        "# data[\"train\"][\"X\"], data[\"val\"][\"X\"], data[\"train\"][\"y\"], data[\"val\"][\"y\"] = train_test_split(data[\"train\"][\"X\"], data[\"train\"][\"y\"],\n",
        "#                                                                                                         random_state=SEED, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478aqnniojd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if sample:\n",
        "    for l in [\"train\", \"test\"]:\n",
        "        for k in [\"X\", \"y\"]:\n",
        "            data[l][k] = data[l][k][:sample+1]\n",
        "\n",
        "# Set the maximum sequence length. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 256\n",
        "# Optional function to find MAX_LEN: "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nqH_6nJ97St",
        "colab_type": "text"
      },
      "source": [
        "## Loading pre-trained BERT weights and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdeZVUWo7gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer,AdamW, get_linear_schedule_with_warmup, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
        "model_class, tokenizer_class, pretrained_weights = BertForSequenceClassification, BertTokenizer, \"bert-base-uncased\" #'bert-base-multilingual-cased' #allenai/longformer-base-4096\n",
        "#tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model_class = \"bert-base-uncased\"#'allenai/longformer-base-4096'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUArAej2TpVP",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization - Max Length Trim/Pad + Encode - BatchLoader -> train/val/test  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APZLu03z0fQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in [\"train\", \"test\"]:\n",
        "    print(\"l: \", l)\n",
        "    data[l][\"tokenized\"] = tqdm([tokenizer.tokenize(text)[:MAX_LEN]\n",
        "                                 for text in data[l][\"X\"]])  # [:MAX_LEN]\n",
        "    data[l][\"input_ids\"] = pad_sequences(tqdm([tokenizer.convert_tokens_to_ids(tokenized) for tokenized in data[l][\"tokenized\"]]),\n",
        "                                         maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    # Create attention masks\n",
        "    data[l][\"masks\"] = []\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in data[l][\"input_ids\"]:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        data[l][\"masks\"].append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K72fG5MeUhek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"train\"][\"inputs\"], data[\"val\"][\"inputs\"], data[\"train\"][\"y\"], data[\"val\"][\"y\"] = train_test_split(data[\"train\"][\"input_ids\"], data[\"train\"][\"y\"],\n",
        "                                                                                                        random_state=SEED, test_size=0.1)\n",
        "data[\"train\"][\"masks\"], data[\"val\"][\"masks\"], _, _ = train_test_split(data[\"train\"][\"masks\"], data[\"train\"][\"input_ids\"],\n",
        "                                                                      random_state=SEED, test_size=0.1)\n",
        "data[\"test\"][\"inputs\"] = data[\"test\"][\"input_ids\"]\n",
        "\n",
        "# TORCHify arrays and matrices\n",
        "for l in [\"train\", \"val\", \"test\"]:\n",
        "    print(l)\n",
        "    for k in [\"inputs\", \"y\", \"masks\"]:\n",
        "        print(k)\n",
        "        data[l][k] = torch.tensor(data[l][k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUw-s5BnUP--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LOADERs\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "train_data = TensorDataset(data[\"train\"][\"inputs\"], data[\"train\"][\"masks\"], data[\"train\"][\"y\"])\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "val_data = TensorDataset(data[\"val\"][\"inputs\"], data[\"val\"][\"masks\"], data[\"val\"][\"y\"])\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(\n",
        "    val_data, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_data = TensorDataset(data[\"test\"][\"inputs\"], data[\"test\"][\"masks\"], data[\"test\"][\"y\"])\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKDwQvfYUXxL",
        "colab_type": "text"
      },
      "source": [
        "## Training & Optimization params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NP1lz_I0ffU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "EPOCHS = 2\n",
        "if sample: EPOCHS = 1\n",
        "lr = 3e-5 #1e-6\n",
        "max_grad_norm = 1.0\n",
        "warmup_proportion = 0.1\n",
        "num_total_steps = len(train_dataloader) #*EPOCHS\n",
        "num_warmup_steps = float(num_total_steps)*0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsix0VY_VPiq",
        "colab_type": "text"
      },
      "source": [
        "## Fire up the model on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYA0OGGHU9pH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = model_class.from_pretrained(pretrained_weights, num_labels=num_labels)\n",
        "config = AutoConfig.from_pretrained(model_class,num_labels=num_labels)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_class,config=config)\n",
        "model.cuda()\n",
        "# To reproduce BertAdam specific behavior set correct_bias=False\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps = num_total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDFP1ajfsa3L",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZqBhhQgsfb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, brier_score_loss, precision_recall_curve,roc_curve,auc,roc_auc_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "def evaluation_measures(gold, predicted, target_names=[]):\n",
        "    acc = accuracy_score(gold,predicted)\n",
        "    print('Accuracy [:( metric ]:', acc)\n",
        "    if target_names:\n",
        "        report = classification_report(gold, predicted, target_names=target_names)\n",
        "        repdict = classification_report(gold, predicted, target_names=target_names, output_dict=True)\n",
        "    else:\n",
        "        report = classification_report(gold, predicted)\n",
        "        repdict = classification_report(gold, predicted, output_dict=True)\n",
        "    print('Classification report:')\n",
        "    print(report)\n",
        "    matrix = confusion_matrix(gold, predicted)\n",
        "    print('Confusion matrix:')\n",
        "    print(matrix)\n",
        "    return repdict, matrix\n",
        "\n",
        "def calc_uof_fp(points, thresh):\n",
        "    stats = {}\n",
        "    stats[\"pos_over\"], stats[\"pos_under\"], stats[\"neg_over\"], stats[\"neg_under\"] = 0, 0, 0, 0\n",
        "\n",
        "    multiply = True\n",
        "    for i, p, value, status, group in points:\n",
        "        if value > 1:\n",
        "            multiply = False\n",
        "            break\n",
        "    if multiply:\n",
        "        points = [(i, p, 100*value, status, group) for i, p, value, status, group in points]\n",
        "\n",
        "    for i, p, value, status, group in points:\n",
        "        if status == True:\n",
        "            if value >= thresh:\n",
        "                stats[\"pos_over\"] += 1\n",
        "            else:\n",
        "                stats[\"pos_under\"] += 1\n",
        "        else:\n",
        "            if value >= thresh:\n",
        "                stats[\"neg_over\"] += 1\n",
        "            else:\n",
        "                stats[\"neg_under\"] += 1\n",
        "    return round((stats[\"pos_over\"] + stats[\"neg_over\"])/len(points), 4), round(stats[\"neg_over\"]/max(1, ((stats[\"pos_over\"] + stats[\"neg_over\"]))), 2), round((stats[\"pos_over\"])/len(points), 4)\n",
        "\n",
        "def easy_calc_uof_fp(predict, probs, gold, thresh):\n",
        "    \"\"\"\n",
        "    LIST all\n",
        "    \"\"\"\n",
        "    total = len(gold)\n",
        "    boolean = np.array([True if predict[i] == gold[i] else False for i in range(0, len(gold))])\n",
        "    unique = sorted(list(set(sorted([int(x) for x in probs]))))\n",
        "    points = list(zip(list(range(0, total)), predict, probs, boolean.tolist(),\n",
        "                      [unique.index(int(prob)) for prob in probs]))\n",
        "    return calc_uof_fp(points, thresh)\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0) # only difference\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX1n0Jc_rlVA",
        "colab_type": "text"
      },
      "source": [
        "## Train - Validate Model & Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKMjB2YNrkyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
        "\n",
        "    # Training\n",
        "\n",
        "    # Set our model to training mode (as opposed to evaluation mode)\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    #from pdb import set_trace; set_trace()\n",
        "    # Train the data for one epoch\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        # print(loss)\n",
        "\n",
        "        #from pdb import set_trace; set_trace()\n",
        "        train_loss_set.append(loss.item())\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Gradient clipping is not in AdamW anymore (so you can use amp without issue)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Update learning rate for next steps\n",
        "        scheduler.step()\n",
        "        # Clear out the gradients (by default they accumulate)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "    # Validation\n",
        "\n",
        "    # Put model in evaluation mode to evaluate loss on the val set\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in val_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Telling the model not to compute or store gradients, saving memory and speeding up val\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "            logits = outputs[0] # PER BATCH!\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"val Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "\n",
        "# Now let's save our model and tokenizer to a directory\n",
        "model.save_pretrained(path+'/models/')\n",
        "tokenizer.save_pretrained(path+'/models/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bP9PPuvVeR",
        "colab_type": "text"
      },
      "source": [
        "## Test - evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHa16XYEbfm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction on test set\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions, probs, true_labels = [], [], []\n",
        "\n",
        "# Predict\n",
        "for batch in test_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits = outputs[0]\n",
        "        prob = F.softmax(logits, dim=1)\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    prob = prob.detach().cpu().numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "    probs.append(prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHbx_fst0fos",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation & Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kWOep3kvZgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
        "matthews_set = []\n",
        "for i in range(len(true_labels)):\n",
        "    matthews = matthews_corrcoef(true_labels[i],\n",
        "                                np.argmax(predictions[i], axis=1).flatten())\n",
        "    matthews_set.append(matthews)\n",
        "#print(matthews_set)\n",
        "\n",
        "#from pdb import set_trace; set_trace()\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_true_labels = [label for batch in true_labels for label in batch]\n",
        "flat_logits = [logits for batch in predictions for logits in batch]\n",
        "flat_probs = [prob for batch in probs for prob in batch]\n",
        "flat_predictions = np.argmax(flat_logits, axis=1).flatten()\n",
        "flat_argmax_probs = [100*flat_probs[i][flat_predictions[i]] for i in range(len(flat_predictions))]\n",
        "\n",
        "print(\"Matthews Coëfficient: \",matthews_corrcoef(flat_true_labels, flat_predictions))\n",
        "\n",
        "evaluation_measures(flat_true_labels, flat_predictions)#,target_names=labels)\n",
        "\n",
        "for thresh in np.arange(10,100,10):\n",
        "    stats = easy_calc_uof_fp(flat_predictions, flat_argmax_probs, flat_true_labels, thresh)\n",
        "    print(thresh, \":\", stats)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi-Bh2oy0cEM",
        "colab_type": "text"
      },
      "source": [
        "# Congratulations, you have now been converted to BERTology! ![alt text](https://i.ytimg.com/vi/odVtLluew-8/maxresdefault.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uzr4v5KZnU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bonus: how to get your model from google drive to local disk\n",
        "# zip folder with model -> download zip (works BUT slow for large models)\n",
        "# https://stackoverflow.com/questions/53581023/google-colab-file-download-failed-to-fetch-error => make sure to enable third-party cookies + possible refresh\n",
        "\n",
        "from google.colab import files\n",
        "!ls \"$path\"\n",
        "#!zip -r \"$path\"/models.zip \"$path\"'/models/'\n",
        "files.download(path+'/models.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rintsQul0c1l",
        "colab_type": "text"
      },
      "source": [
        "This code and tutorial all borrows insights from the below sources: \n",
        "\n",
        "# Tutorials #\n",
        "\n",
        "https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "https://medium.com/dsnet/running-pytorch-transformers-on-custom-datasets-717fd9e10fe2\n",
        "https://engineering.wootric.com/when-bert-meets-pytorch\n",
        "https://towardsdatascience.com/distilling-bert-models-with-spacy-277c7edc426c\n",
        "https://github.com/huggingface/pytorch-transformers#quick-tour-of-the-fine-tuningusage-scripts\n",
        "https://github.com/explosion/spacy-pytorch-transformers\n",
        "https://github.com/huggingface/pytorch-transformers\n",
        "https://github.com/fredriko/bert-tensorflow-pytorch-spacy-conversion\n",
        "https://arxiv.org/pdf/1905.05583.pdf #how to finetune\n",
        "https://www.kaggle.com/sharmilaupadhyaya/20newsgroup-classification-using-keras-bert-in-gpu \n",
        "https://colab.research.google.com/drive/1YSfscbb-g92m1vkYxY4IOVMWMfgfLLJD #tensorflow version on TPU\n",
        "https://colab.research.google.com/drive/1pS-eegmUz9EqXJw22VbVIHlHoXjNaYuc#scrollTo=JggjeDC9m2MH #BertViz repo\n",
        "https://www.kaggle.com/criscastromaya/cnn-for-nlp-in-keras [compare with CNN]\n",
        "\n",
        "https://github.com/sugi-chan/custom_bert_pipeline/blob/master/bert_pipeline.ipynb #phased method\n",
        "\n",
        "https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/100661 #faster batch training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9A-pVZW0iiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}